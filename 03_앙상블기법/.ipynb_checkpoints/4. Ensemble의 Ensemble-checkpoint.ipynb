{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20984b7a",
   "metadata": {},
   "source": [
    "# Ensemble의 Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b233381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f525462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Hongjin\\\\Desktop\\\\김다윗\\\\개인\\\\01. 공부\\\\Git\\\\ML_code\\\\03_앙상블기법'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 현재경로 확인\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe22bad",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b02871c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_85</th>\n",
       "      <th>feat_86</th>\n",
       "      <th>feat_87</th>\n",
       "      <th>feat_88</th>\n",
       "      <th>feat_89</th>\n",
       "      <th>feat_90</th>\n",
       "      <th>feat_91</th>\n",
       "      <th>feat_92</th>\n",
       "      <th>feat_93</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Class_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  feat_9  \\\n",
       "0   1       1       0       0       0       0       0       0       0       0   \n",
       "1   2       0       0       0       0       0       0       0       1       0   \n",
       "2   3       0       0       0       0       0       0       0       1       0   \n",
       "3   4       1       0       0       1       6       1       5       0       0   \n",
       "4   5       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   ...  feat_85  feat_86  feat_87  feat_88  feat_89  feat_90  feat_91  \\\n",
       "0  ...        1        0        0        0        0        0        0   \n",
       "1  ...        0        0        0        0        0        0        0   \n",
       "2  ...        0        0        0        0        0        0        0   \n",
       "3  ...        0        1        2        0        0        0        0   \n",
       "4  ...        1        0        0        0        0        1        0   \n",
       "\n",
       "   feat_92  feat_93   target  \n",
       "0        0        0  Class_1  \n",
       "1        0        0  Class_1  \n",
       "2        0        0  Class_1  \n",
       "3        0        0  Class_1  \n",
       "4        0        0  Class_1  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../00_Data/otto_train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05736773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid: 고유 아이디\\nfeat_1 ~ feat_93: 설명변수\\ntarget: 타겟변수 (1~9)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "id: 고유 아이디\n",
    "feat_1 ~ feat_93: 설명변수\n",
    "target: 타겟변수 (1~9)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f465f403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nCar: 61878 nVar: 95\n"
     ]
    }
   ],
   "source": [
    "nCar = data.shape[0] # 데이터 개수\n",
    "nVar = data.shape[1] # 변수 개수\n",
    "print('nCar: %d' % nCar, 'nVar: %d' % nVar )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f85d62",
   "metadata": {},
   "source": [
    "### 의미없는 변수 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5753951",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a284372",
   "metadata": {},
   "source": [
    "### 타겟변수 카테고리화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c100795",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {'Class_1':1,\n",
    "                'Class_2':2,\n",
    "                'Class_3':3,\n",
    "                'Class_4':4,\n",
    "                'Class_5':5,\n",
    "                'Class_6':6,\n",
    "                'Class_7':7,\n",
    "                'Class_8':8,\n",
    "                'Class_9':9,}\n",
    "\n",
    "after_mapping_target = data['target'].apply(lambda x:mapping_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf4ef5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_mapping_target[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899585ca",
   "metadata": {},
   "source": [
    "### 학습 및 평가 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c00c674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49502, 93) (12376, 93) (49502,) (12376,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(data.columns.difference(['target']))\n",
    "X = data[feature_columns]\n",
    "y = after_mapping_target\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42) # 학습데이터와 평가데이터의 비율을 8:2 로 분할| \n",
    "print(train_x.shape, test_x.shape, train_y.shape, test_y.shape) # 데이터 개수 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f4e88",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27e637aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.1-py3-none-win_amd64.whl (89.1 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from xgboost) (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from xgboost) (1.20.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f75cbdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:09:47] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-03de431ba26204c4d-1/xgboost/xgboost-ci-windows/src/learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "Accuracy: 76.67 %\n",
      "Wall time: 6.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb_dtrain = xgb.DMatrix(data=train_x, label=train_y) # 학습데이터를 XGBoost 모델에 맞게 변환\n",
    "xgb_dtest = xgb.DMatrix(data=test_x) # 평가데이터도 변환\n",
    "\n",
    "xgb_param = {'max_depth':10, # 트리 깊이\n",
    "             'learning_rate':0.01, # step size\n",
    "             'n_estimators':100, # 트리 생성 개수\n",
    "             'objective':'multi:softmax', # 목적함수\n",
    "             'num_class':len(set(train_y))+1} # num_class 보다 1 커야함\n",
    "\n",
    "xgb_model = xgb.train(params=xgb_param, dtrain=xgb_dtrain) # 학습\n",
    "xgb_model_predict = xgb_model.predict(xgb_dtest) # 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, xgb_model_predict)*100), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c05d60",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a383fef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from lightgbm) (1.1.1)\n",
      "Requirement already satisfied: wheel in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from lightgbm) (0.37.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from lightgbm) (1.20.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from lightgbm) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28d134fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hongjin\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008317 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3110\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score -34.538776\n",
      "[LightGBM] [Info] Start training from score -3.476745\n",
      "[LightGBM] [Info] Start training from score -1.341381\n",
      "[LightGBM] [Info] Start training from score -2.039019\n",
      "[LightGBM] [Info] Start training from score -3.135151\n",
      "[LightGBM] [Info] Start training from score -3.125444\n",
      "[LightGBM] [Info] Start training from score -1.481556\n",
      "[LightGBM] [Info] Start training from score -3.074772\n",
      "[LightGBM] [Info] Start training from score -1.986562\n",
      "[LightGBM] [Info] Start training from score -2.533374\n",
      "Accuracy: 76.28 %\n",
      "Wall time: 3.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import lightgbm as lgb\n",
    "\n",
    "lgb_dtrain = lgb.Dataset(data=train_x, label=train_y) # 학습데이터를 LightGBM 모델에 맞게 변환\n",
    "\n",
    "lgb_param = {'max_depth':10, # 트리 깊이\n",
    "             'learning_rate':0.01, # Step size\n",
    "             'n_estimators':100, # 트리 생성 개수\n",
    "             'objective':'multiclass', # 목적함수\n",
    "             'num_class':len(set(train_y))+1} # num_class 보다 1 커야함\n",
    "\n",
    "lgb_model = lgb.train(params=lgb_param, train_set=lgb_dtrain) # 학습\n",
    "lgb_model_predict = np.argmax(lgb_model.predict(test_x), axis=1) # 예측\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, lgb_model_predict)*100), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d8da5",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cff3b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.1.1-cp39-none-win_amd64.whl (74.0 MB)\n",
      "Collecting plotly\n",
      "  Downloading plotly-5.11.0-py2.py3-none-any.whl (15.3 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from catboost) (1.7.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from catboost) (1.3.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from catboost) (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from catboost) (1.20.3)\n",
      "Requirement already satisfied: six in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hongjin\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (8.4.0)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.1.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: tenacity, plotly, catboost\n",
      "Successfully installed catboost-1.1.1 plotly-5.11.0 tenacity-8.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8360e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5907034\ttotal: 531ms\tremaining: 52.5s\n",
      "1:\tlearn: 0.6356107\ttotal: 957ms\tremaining: 46.9s\n",
      "2:\tlearn: 0.6411256\ttotal: 1.5s\tremaining: 48.4s\n",
      "3:\tlearn: 0.6480344\ttotal: 1.95s\tremaining: 46.9s\n",
      "4:\tlearn: 0.6508222\ttotal: 2.41s\tremaining: 45.8s\n",
      "5:\tlearn: 0.6499939\ttotal: 2.87s\tremaining: 45s\n",
      "6:\tlearn: 0.6507818\ttotal: 3.31s\tremaining: 44s\n",
      "7:\tlearn: 0.6548422\ttotal: 3.74s\tremaining: 43s\n",
      "8:\tlearn: 0.6559533\ttotal: 4.16s\tremaining: 42.1s\n",
      "9:\tlearn: 0.6560947\ttotal: 4.57s\tremaining: 41.1s\n",
      "10:\tlearn: 0.6568421\ttotal: 5s\tremaining: 40.5s\n",
      "11:\tlearn: 0.6588219\ttotal: 5.47s\tremaining: 40.1s\n",
      "12:\tlearn: 0.6592259\ttotal: 5.91s\tremaining: 39.5s\n",
      "13:\tlearn: 0.6611248\ttotal: 6.32s\tremaining: 38.8s\n",
      "14:\tlearn: 0.6625591\ttotal: 6.73s\tremaining: 38.1s\n",
      "15:\tlearn: 0.6631853\ttotal: 7.16s\tremaining: 37.6s\n",
      "16:\tlearn: 0.6639328\ttotal: 7.58s\tremaining: 37s\n",
      "17:\tlearn: 0.6668821\ttotal: 8s\tremaining: 36.4s\n",
      "18:\tlearn: 0.6669630\ttotal: 8.37s\tremaining: 35.7s\n",
      "19:\tlearn: 0.6675286\ttotal: 8.77s\tremaining: 35.1s\n",
      "20:\tlearn: 0.6673266\ttotal: 9.15s\tremaining: 34.4s\n",
      "21:\tlearn: 0.6677104\ttotal: 9.53s\tremaining: 33.8s\n",
      "22:\tlearn: 0.6682558\ttotal: 9.92s\tremaining: 33.2s\n",
      "23:\tlearn: 0.6683972\ttotal: 10.3s\tremaining: 32.7s\n",
      "24:\tlearn: 0.6686599\ttotal: 10.7s\tremaining: 32.2s\n",
      "25:\tlearn: 0.6681952\ttotal: 11.1s\tremaining: 31.7s\n",
      "26:\tlearn: 0.6684982\ttotal: 11.5s\tremaining: 31.2s\n",
      "27:\tlearn: 0.6692053\ttotal: 11.9s\tremaining: 30.7s\n",
      "28:\tlearn: 0.6696699\ttotal: 12.3s\tremaining: 30.2s\n",
      "29:\tlearn: 0.6699325\ttotal: 12.8s\tremaining: 29.9s\n",
      "30:\tlearn: 0.6705992\ttotal: 13.2s\tremaining: 29.4s\n",
      "31:\tlearn: 0.6709426\ttotal: 13.6s\tremaining: 28.9s\n",
      "32:\tlearn: 0.6708012\ttotal: 14s\tremaining: 28.4s\n",
      "33:\tlearn: 0.6709426\ttotal: 14.4s\tremaining: 28s\n",
      "34:\tlearn: 0.6707002\ttotal: 14.8s\tremaining: 27.5s\n",
      "35:\tlearn: 0.6715082\ttotal: 15.2s\tremaining: 27.1s\n",
      "36:\tlearn: 0.6705992\ttotal: 15.6s\tremaining: 26.6s\n",
      "37:\tlearn: 0.6725991\ttotal: 16s\tremaining: 26.1s\n",
      "38:\tlearn: 0.6729829\ttotal: 16.4s\tremaining: 25.6s\n",
      "39:\tlearn: 0.6725991\ttotal: 16.8s\tremaining: 25.1s\n",
      "40:\tlearn: 0.6734273\ttotal: 17.1s\tremaining: 24.7s\n",
      "41:\tlearn: 0.6738314\ttotal: 17.5s\tremaining: 24.2s\n",
      "42:\tlearn: 0.6741546\ttotal: 17.9s\tremaining: 23.8s\n",
      "43:\tlearn: 0.6739728\ttotal: 18.3s\tremaining: 23.3s\n",
      "44:\tlearn: 0.6741950\ttotal: 18.7s\tremaining: 22.9s\n",
      "45:\tlearn: 0.6750636\ttotal: 19.1s\tremaining: 22.4s\n",
      "46:\tlearn: 0.6758919\ttotal: 19.4s\tremaining: 21.9s\n",
      "47:\tlearn: 0.6757707\ttotal: 19.8s\tremaining: 21.5s\n",
      "48:\tlearn: 0.6762151\ttotal: 20.2s\tremaining: 21s\n",
      "49:\tlearn: 0.6774474\ttotal: 20.6s\tremaining: 20.6s\n",
      "50:\tlearn: 0.6777100\ttotal: 21s\tremaining: 20.2s\n",
      "51:\tlearn: 0.6786594\ttotal: 21.4s\tremaining: 19.8s\n",
      "52:\tlearn: 0.6789827\ttotal: 21.8s\tremaining: 19.3s\n",
      "53:\tlearn: 0.6804372\ttotal: 22.2s\tremaining: 18.9s\n",
      "54:\tlearn: 0.6804372\ttotal: 22.5s\tremaining: 18.4s\n",
      "55:\tlearn: 0.6809220\ttotal: 22.9s\tremaining: 18s\n",
      "56:\tlearn: 0.6812250\ttotal: 23.3s\tremaining: 17.6s\n",
      "57:\tlearn: 0.6813058\ttotal: 23.7s\tremaining: 17.2s\n",
      "58:\tlearn: 0.6811846\ttotal: 24.1s\tremaining: 16.8s\n",
      "59:\tlearn: 0.6813260\ttotal: 24.5s\tremaining: 16.3s\n",
      "60:\tlearn: 0.6816694\ttotal: 24.9s\tremaining: 15.9s\n",
      "61:\tlearn: 0.6823159\ttotal: 25.3s\tremaining: 15.5s\n",
      "62:\tlearn: 0.6832653\ttotal: 25.7s\tremaining: 15.1s\n",
      "63:\tlearn: 0.6840734\ttotal: 26s\tremaining: 14.7s\n",
      "64:\tlearn: 0.6840734\ttotal: 26.4s\tremaining: 14.2s\n",
      "65:\tlearn: 0.6846592\ttotal: 26.8s\tremaining: 13.8s\n",
      "66:\tlearn: 0.6843360\ttotal: 27.2s\tremaining: 13.4s\n",
      "67:\tlearn: 0.6846390\ttotal: 27.6s\tremaining: 13s\n",
      "68:\tlearn: 0.6854269\ttotal: 28s\tremaining: 12.6s\n",
      "69:\tlearn: 0.6858309\ttotal: 28.4s\tremaining: 12.2s\n",
      "70:\tlearn: 0.6858309\ttotal: 28.8s\tremaining: 11.8s\n",
      "71:\tlearn: 0.6865783\ttotal: 29.2s\tremaining: 11.3s\n",
      "72:\tlearn: 0.6864167\ttotal: 29.5s\tremaining: 10.9s\n",
      "73:\tlearn: 0.6868611\ttotal: 29.9s\tremaining: 10.5s\n",
      "74:\tlearn: 0.6869217\ttotal: 30.3s\tremaining: 10.1s\n",
      "75:\tlearn: 0.6870429\ttotal: 30.7s\tremaining: 9.7s\n",
      "76:\tlearn: 0.6875278\ttotal: 31.1s\tremaining: 9.3s\n",
      "77:\tlearn: 0.6881136\ttotal: 31.5s\tremaining: 8.89s\n",
      "78:\tlearn: 0.6883762\ttotal: 31.9s\tremaining: 8.49s\n",
      "79:\tlearn: 0.6888207\ttotal: 32.3s\tremaining: 8.09s\n",
      "80:\tlearn: 0.6892449\ttotal: 32.8s\tremaining: 7.69s\n",
      "81:\tlearn: 0.6898509\ttotal: 33.2s\tremaining: 7.28s\n",
      "82:\tlearn: 0.6897095\ttotal: 33.5s\tremaining: 6.87s\n",
      "83:\tlearn: 0.6902549\ttotal: 33.9s\tremaining: 6.46s\n",
      "84:\tlearn: 0.6909822\ttotal: 34.3s\tremaining: 6.05s\n",
      "85:\tlearn: 0.6910832\ttotal: 34.7s\tremaining: 5.65s\n",
      "86:\tlearn: 0.6914468\ttotal: 35.1s\tremaining: 5.25s\n",
      "87:\tlearn: 0.6916084\ttotal: 35.5s\tremaining: 4.84s\n",
      "88:\tlearn: 0.6919922\ttotal: 35.9s\tremaining: 4.43s\n",
      "89:\tlearn: 0.6925579\ttotal: 36.3s\tremaining: 4.03s\n",
      "90:\tlearn: 0.6928407\ttotal: 36.6s\tremaining: 3.62s\n",
      "91:\tlearn: 0.6930427\ttotal: 37s\tremaining: 3.22s\n",
      "92:\tlearn: 0.6935073\ttotal: 37.4s\tremaining: 2.81s\n",
      "93:\tlearn: 0.6940932\ttotal: 37.8s\tremaining: 2.41s\n",
      "94:\tlearn: 0.6944972\ttotal: 38.2s\tremaining: 2.01s\n",
      "95:\tlearn: 0.6948810\ttotal: 38.5s\tremaining: 1.6s\n",
      "96:\tlearn: 0.6951840\ttotal: 39s\tremaining: 1.21s\n",
      "97:\tlearn: 0.6954264\ttotal: 39.6s\tremaining: 809ms\n",
      "98:\tlearn: 0.6955881\ttotal: 40s\tremaining: 404ms\n",
      "99:\tlearn: 0.6956285\ttotal: 40.4s\tremaining: 0us\n",
      "Accuracy: 69.64 %\n",
      "Wall time: 40.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import catboost as cb\n",
    "\n",
    "cb_dtrain = cb.Pool(data=train_x, label=train_y) # 학습데이터를 Catboost 모델에 맞게 변환\n",
    "\n",
    "cb_param = {'max_depth':10, # 트리 깊이\n",
    "            'learning_rate':0.01, # Step size\n",
    "            'n_estimators':100, # 트리 생성 개수\n",
    "            'eval_metric':'Accuracy', # 평가 척도\n",
    "            'loss_function':'MultiClass'} # 손실함수, 목적함수\n",
    "\n",
    "cb_model = cb.train(pool=cb_dtrain, params=cb_param) # 학습\n",
    "cb_model_predict = np.argmax(cb_model.predict(test_x), axis=1) + 1 # 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측, 인덱스의 순서를 맞추기 위해 +1\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, cb_model_predict)*100), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a92ced5",
   "metadata": {},
   "source": [
    "## Ensemble의 Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cb6c478",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hongjin\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007818 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2966\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score 4.858248\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2961\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score 4.831239\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2958\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score 4.842229\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2990\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score 4.836693\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2966\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score 4.832350\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.014496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2974\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score 4.819886\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012372 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2954\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score 4.821341\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2961\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score 4.847461\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2988\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score 4.842855\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2960\n",
      "[LightGBM] [Info] Number of data points in the train set: 49502, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score 4.837885\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import random\n",
    "\n",
    "sample = 10\n",
    "bagging_predict_result = []\n",
    "for _ in range(sample):\n",
    "    data_index = [data_index for data_index in range(train_x.shape[0])]\n",
    "    random_index = np.random.choice(data_index, train_x.shape[0])\n",
    "    \n",
    "    lgb_dtrain = lgb.Dataset(data=train_x.iloc[random_index, :], label=train_y.iloc[random_index])\n",
    "    lgb_param = {'max_depth':14,\n",
    "                 'learning_rate':0.01,\n",
    "                 'n_estimators':500,\n",
    "                 'objective':'regression'}\n",
    "    \n",
    "    lgb_model = lgb.train(params=lgb_param, train_set=lgb_dtrain)\n",
    "    predict1 = lgb_model.predict(test_x)\n",
    "    bagging_predict_result.append(predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f153e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_predict = []\n",
    "for idx_test in range(test_x.shape[0]):\n",
    "    temp_predict = []\n",
    "    for idx in range(len(bagging_predict_result)):\n",
    "        temp_predict.append(bagging_predict_result[idx][idx_test])\n",
    "    bagging_predict.append(np.mean(temp_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d2b0177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.64 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.2f\" % (accuracy_score(test_y, cb_model_predict)*100), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
